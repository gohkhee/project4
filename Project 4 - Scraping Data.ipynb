{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal has two main objectives:\n",
    "\n",
    "   1. Determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "   2. Determine the factors that distinguish job categories and titles from each other. For example, can required skills accurately predict job title?\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL we want to visit.\n",
    "url = \"https://www.mycareersfuture.sg/\"\n",
    "\n",
    "# Visit the URL and grab the HTML of the page.\n",
    "html = urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver = \"/Users/edoardo/github_dsi4/classes/week-06/labs/python-webscraping_opentable-lab-master/chromedriver/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "# driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function that will extract all the relevant job details from each page and return a dictionary with all the relevant\n",
    "#information\n",
    "\n",
    "def info_extract(link, location):\n",
    "    \n",
    "    #go the the webpage indicated by the link\n",
    "    \n",
    "    driver.get(link)\n",
    "    sleep(6)\n",
    "    \n",
    "#     Eventually we want to convert this a list of dictionaries into a pandas dataframe\n",
    "#     job_df = pd.DataFrame(columns=[\"Title\",\"Company\",\"Location\",\"Applications\",\n",
    "#                                  \"EmployType\", \"Seniority\",\"Categories\", \"SalaryLow\",\n",
    "#                                  \"SalaryHigh\", \"PostedDate\", \"ExpiryDate\" , \"Requirements\"])\n",
    "\n",
    "    #initialise job dictionary\n",
    "    job_dict = {}\n",
    "    #get job title\n",
    "    try:\n",
    "        title = driver.find_element_by_id(\"job_title\").text\n",
    "    except:\n",
    "        title = np.nan\n",
    "        \n",
    "    try:\n",
    "        company = driver.find_element_by_name(\"company\").text\n",
    "    except:\n",
    "        company = np.nan\n",
    "  \n",
    "    try:\n",
    "        employtype = driver.find_element_by_id(\"employment_type\").text\n",
    "    except:\n",
    "        employtype = np.nan #put null value if this field cannot be found\n",
    "        \n",
    "    try:\n",
    "        senior = driver.find_element_by_id(\"seniority\").text\n",
    "    except:\n",
    "        senior = np.nan  #put null value if this field cannot be found\n",
    "        \n",
    "    #check if we want to split these by commas\n",
    "    \n",
    "    try:\n",
    "        category = driver.find_element_by_id(\"job-categories\").text\n",
    "    except:\n",
    "        category = np.nan \n",
    "    \n",
    "    try:\n",
    "        applications = driver.find_element_by_id(\"num_of_applications\").text.replace(' application','').replace('s','')\n",
    "    except:\n",
    "        applications = np.nan \n",
    "        \n",
    "    #get the text from job requirements\n",
    "    \n",
    "    #get income range\n",
    "    try:\n",
    "        income_path = '//*[@id=\"job_details\"]/div[1]/div[2]/div[1]/div/section[2]/div/span[2]/div'\n",
    "        income_range = driver.find_element_by_xpath(income_path).text.split('to')\n",
    "        income_lower = income_range[0]\n",
    "        income_upper = income_range[1]\n",
    "        \n",
    "    except:\n",
    "        income_range = np.nan \n",
    "        income_upper = np.nan \n",
    "        income_lower = np.nan \n",
    "    \n",
    "    try:\n",
    "        posted = driver.find_element_by_id(\"last_posted_date\").text\n",
    "    except:\n",
    "        posted = np.nan    \n",
    "        \n",
    "    try:\n",
    "        expiry = driver.find_element_by_id(\"expiry_date\").text\n",
    "    except:\n",
    "        expiry = np.nan    \n",
    "    \n",
    "    try:\n",
    "        #req_path = '//*[@id=\"requirements-content\"]/ul/li[1]/p'\n",
    "        #reqts = driver.find_element_by_xpath(req_path).text\n",
    "        reqts = driver.find_element_by_id(\"requirements-content\").text\n",
    "        \n",
    "    except:\n",
    "        reqts = np.nan    \n",
    "    \n",
    "\n",
    "    rate_path = '//*[@id=\"job_details\"]/div[1]/div[2]/div[1]/div/section[2]/div/span[3]'\n",
    "    try:\n",
    "        income_rate = driver.find_element_by_xpath(rate_path).text\n",
    "    except:\n",
    "        income_rate = np.nan\n",
    "    \n",
    "        \n",
    "        \n",
    "    #update the dictionary\n",
    "    job_dict['Title'] = title\n",
    "    job_dict['Company'] = company\n",
    "    job_dict['EmployType'] = employtype\n",
    "    job_dict['Seniority'] = senior\n",
    "    job_dict['Company'] = company\n",
    "    job_dict['Categories'] = category\n",
    "    job_dict['Applications'] = applications\n",
    "    job_dict['SalaryLow'] = income_lower\n",
    "    job_dict['SalaryHigh'] = income_upper\n",
    "    job_dict['PostedDate'] = posted\n",
    "    job_dict['ExpiryDate'] = expiry\n",
    "    job_dict['Requirements'] = reqts\n",
    "    job_dict['Location'] = location\n",
    "    job_dict['SalaryRate'] = income_rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    return job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function that will visit each of the href links on each page in the job listings\n",
    "\n",
    "def get_links():\n",
    "  \n",
    "    list_links = []\n",
    "    jobcards = []\n",
    "    loc_list = []\n",
    "    location_list = []\n",
    "    jobcards = []\n",
    "    \n",
    "    #for each job card on the search results\n",
    "    for i in range(0,20):\n",
    "        jobcard_xpath_iter = '//*[@id=\"job-card-%d\"]/div/a'%i\n",
    "        jobcard = driver.find_element_by_xpath(jobcard_xpath_iter)\n",
    "        jobcards.append(jobcard)\n",
    "        \n",
    "        loc_xpath = '//*[@id=\"job-card-%d\"]/div/a/div[1]/div[1]/section/div[2]/div[2]/section/p[1]'%i\n",
    "        loc_elem = driver.find_element_by_xpath(loc_xpath)  \n",
    "        location_list.append(loc_elem)\n",
    "\n",
    "  \n",
    "    #location_list = driver.find_elements_by_name(\"location\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    for location in location_list:\n",
    "        #retrieve link\n",
    "        try:\n",
    "            loc_text = location.text\n",
    "            loc_list.append(loc_text)\n",
    "            #print(\"size of loc_list is \", len(loc_list))\n",
    "        except:\n",
    "            #if no text found\n",
    "            loc_text = np.nan\n",
    "            loc_list.append(loc_text)\n",
    "            \n",
    "            \n",
    "\n",
    "    #get href link\n",
    "    for job in jobcards:\n",
    "        #retrieve link\n",
    "        joblink = job.get_attribute('href')\n",
    "        #append to the list of links\n",
    "        list_links.append(joblink)\n",
    "        \n",
    "    return list_links, loc_list\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver called \"driver.\"\n",
    "# Visit the relevant page\n",
    "driver = webdriver.Chrome(executable_path=\"C:/Users/Kai Hee/materials/projects/project-4/chromedriver/chromedriver\")\n",
    "driver.get(\"https://www.mycareersfuture.sg/\")\n",
    "\n",
    "\n",
    "sleep(5)\n",
    "# Grab the page source.\n",
    "html = driver.page_source\n",
    "\n",
    "#find the search bar\n",
    "searchbar = driver.find_element_by_name(\"search-text\")\n",
    "\n",
    "#enter search term and get results page\n",
    "searchbar.send_keys(\"data\")\n",
    "searchbar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait five seconds.\n",
    "sleep(5)\n",
    "\n",
    "i = 0\n",
    "\n",
    "urls = []\n",
    "job_links = []\n",
    "location_list = []\n",
    "dict_list = []\n",
    "\n",
    "#we take the first 220 pages of search results as they are likely the more relevant ones\n",
    "#runs = 220\n",
    "runs = 220\n",
    "\n",
    "\n",
    "pbar = tqdm(total = runs+1)\n",
    "\n",
    "while(i<runs):#lets try for a few pages first\n",
    "   \n",
    "    try:\n",
    "        #retrieve the next page of search results\n",
    "        driver.get(\"https://www.mycareersfuture.sg/search?search=data&sortBy=new_posting_date&page=\"+str(i))\n",
    "        sleep(5)\n",
    "        \n",
    "        #get the links of all the job listings on the page\n",
    "        new_links, locations = get_links()\n",
    "        \n",
    "        #concatenate to full job list\n",
    "        job_links = job_links + new_links\n",
    "        location_list = location_list + locations\n",
    "        \n",
    "        i = i+1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        #for each page, print statement when links have been retrieved\n",
    "        print(\"Job links retrieved:\", len(job_links))\n",
    "        \n",
    "    except:\n",
    "        print(\"Oops\")\n",
    "        break\n",
    "        \n",
    "\n",
    "#once we have gotten all the relevant links, we visit each page to extract the required job info\n",
    "\n",
    "loc_index = 0\n",
    "batch = 0\n",
    "\n",
    "for job in job_links:#for each link on the search results\n",
    "    \n",
    "    #extract information and tag location info to this dictionary\n",
    "    job_dict ={}\n",
    "    job_dict = info_extract(job, location_list[loc_index])\n",
    "    dict_list.append(job_dict)\n",
    "    loc_index = loc_index + 1\n",
    "\n",
    "    #once all job data has been added to the job dictionary, save as a dataframe\n",
    "\n",
    "    #update batch number for every 45 pages of search results\n",
    "\n",
    "    if (loc_index%900 == 0):#for every 20 x 450 listings\n",
    "        \n",
    "        batch = batch + 1\n",
    "\n",
    "        job_df = pd.DataFrame(dict_list)\n",
    "\n",
    "        #export to csv file\n",
    "\n",
    "        job_df.to_csv(\"jobs\"+ str(batch)+ \".csv\")\n",
    "        \n",
    "        #reset the dict_list\n",
    "        del dict_list[:]\n",
    "        \n",
    "\n",
    "#checking that all the job info was saved correctly\n",
    "\n",
    "#for job_info in dict_list:\n",
    "#     print(job_info['Title'])\n",
    "#     print(job_info['Company'])\n",
    "#     print(job_info['EmployType'])\n",
    "#     print(job_info['Seniority'])\n",
    "#     print(job_info['Company'])\n",
    "#     print(job_info['Categories'])\n",
    "#     print(job_info['Applications'])\n",
    "#     print(job_info['SalaryLow']) \n",
    "#     print(job_info['SalaryHigh'])\n",
    "#     print(job_info['SalaryRate'])\n",
    "#     print(job_info['PostedDate'])\n",
    "#     print(job_info['ExpiryDate'])\n",
    "#     print(job_info['Requirements'])\n",
    "#     print(job_info['Location'])\n",
    "\n",
    "    \n",
    "    \n",
    "# for loc in location_list:\n",
    "#     print(loc)\n",
    "\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "# \n",
    "# Beautiful Soup it!\n",
    "\n",
    "#html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close it.\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last few listings may be a slightly smaller batch. export remaining listings to csv file\n",
    "\n",
    "job_df = pd.DataFrame(dict_list)\n",
    "#export to csv file\n",
    "\n",
    "job_df.to_csv(\"jobs5.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
